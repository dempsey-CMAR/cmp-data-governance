[
  {
    "objectID": "pages/cmp_overview.html",
    "href": "pages/cmp_overview.html",
    "title": "Coastal Monitoring Program",
    "section": "",
    "text": "The Centre for Marine Applied Research’s (CMAR) Coastal Monitoring Program is an ongoing data collection effort that aims to support and inform science-based development of Nova Scotia’s marine coastal industry, guide government policy and management decisions, encourage environmental stewardship, and ensure preparedness for climate change.\nThis website is part of CMAR’s Data Governance initiatives, and provides technical information on Quality Control applied to the Water Quality data. This website is under development, and content will be revised and added.\nTo return to CMAR’s main Coastal Monitoring Program website page, click [here]."
  },
  {
    "objectID": "pages/do_measurements.html",
    "href": "pages/do_measurements.html",
    "title": "CMAR Measurements",
    "section": "",
    "text": "CMAR has collected dissolved oxygen data from 63 stations in 13 counties (Figure 1).\n\n\n\n\n\nFigure 1: Approximate location of stations with dissolved oxygen data. At most stations, dissolved oxygen is measured in units of percent saturation. For several stations in Halifax and Lunenburg counties, dissolved oxygen is now recorded in units of concentration.\n\n\n\n\n\n\nA large proportion of these records are from Guysborough County (23.37 %) and Shelburne County (20.58 %), while a small proportion are from Antigonish (0.53 %) and Pictou (1.1 %) Counties (Figure 2).\n\n\n\n\n\nFigure 2: The number of dissolved oxygen observations in each county."
  },
  {
    "objectID": "pages/do_measurements.html#depth",
    "href": "pages/do_measurements.html#depth",
    "title": "CMAR Measurements",
    "section": "Depth",
    "text": "Depth\nThere is typically one DO sensor on each sensor string, usually attached 5 m below the surface (Figure 3). This choice of depth reflects the original Coastal Monitoring Program objective, which was to provide data to inform aquaculture site selection1. As the Program and mandate expands, DO sensors continue to be deployed at this depth for consistency and longevity of the time series.\nDissolved oxygen has also been measured at other depths for specific research projects, particularly in Whycocomagh Basin, Inverness County (link to Inverness report) (Figure 3).\n\n\n\n\n\nFigure 3: Number of dissolved oxygen observations at each depth (rounded to nearest whole number). Note that only depths with measurements are shown."
  },
  {
    "objectID": "pages/do_measurements.html#sensors",
    "href": "pages/do_measurements.html#sensors",
    "title": "CMAR Measurements",
    "section": "Sensors",
    "text": "Sensors\nCMAR uses two types of sensors to measure DO: the aquaMeasure DOT (InnovaSea 2021) and Onset HOBO U26 (Onset 2012).\n\naquaMeasure DOT\nCMAR has a large inventory of DOTs (71 in April 2023), which are used for most deployments with DO. The DOT measures temperature and partial pressure of oxygen, providing DO values in units of percent saturation. Partial pressure values are not affected by salinity (Bittig et al. 2018), and so no correction factor is required for these measurements. For CMAR deployments, each DOT is programmed to measure and internally log at 10 minute intervals. CMAR typically retrieves sensors after ~1 year for data offload, cleaning, and calibration/validation.\n\nInnovaSea water vapour cal does not include salinity (assumes 0)\n\n\n\nHOBO U26\nCMAR supplemented their DO sensor inventory with 13 HOBO U26 sensors in 2021, and purchased an additional 10 in January 2023. The HOBO U26 measures concentration, and provides DO vales in units of mg / L. For CMAR deployments, each HOBO U26 is programmed to recorded temperature and dissolved oxygen every 10 minutes. These sensors have a sensor cap that needs to be replaced every 6 months. They are deployed in St. Margaret’s Bay and Mahone Bay rather than more remote locations so that they can be serviced twice a year with minimal travel requirements (Figure 1).\nThe HOBO U26 sensor does not account for salinity, so the measurements should be adjusted based on a salinity correction factor before analysis. The HOBOware software can apply this salinity correction if salinity conditions are provided. HOBOware can also convert measured concentration values to percent saturation if salinity and pressure conditions are provided. The CMAR R package docalcs provides functions for those calculations; however, CMAR does not have consistent salinity or pressure measurements, and so the uncorrected data are provided."
  },
  {
    "objectID": "pages/do_measurements.html#biofouling",
    "href": "pages/do_measurements.html#biofouling",
    "title": "CMAR Measurements",
    "section": "Biofouling",
    "text": "Biofouling\nThe DOT and HOBO U26 do not have any anti-fouling mechanisms, and so measurements can be susceptible to biofouling. Ideally, sensors would be cleaned every 2 - 4 weeks to remove fouling, but this is not feasible due to logistical and financial constraints. Significant effort was made to identify and flag possible biofouling signals, although this is challenging for the reasons noted here."
  },
  {
    "objectID": "pages/do_overview.html",
    "href": "pages/do_overview.html",
    "title": "Overview",
    "section": "",
    "text": "Dissolved oxygen (DO) is a measure of the amount of gaseous oxygen dissolved in water, which is a key water quality parameter for much aquatic life (Santana et al. 2017). There are many drivers of DO variability, including biological activity (e.g., photosynthesis, respiration, plankton blooms), physical processes (e.g., air-sea exchange, tidal events, meteorology, seasonal stratification), and chemical reactions (e.g., oxidation of organic material) (Santana et al. 2017; IOOS 2018). DO is typically higher near the surface, where it is produced by photosynthetic organisms and is in flux to maintain equilibrium with the atmosphere. DO is distributed to deeper waters by vertical mixing and diffusion."
  },
  {
    "objectID": "pages/do_overview.html#units",
    "href": "pages/do_overview.html#units",
    "title": "Overview",
    "section": "Units",
    "text": "Units\nDissolved oxygen can be measured in different units, including concentration and percent saturation (Bittig 2018). For fisheries and aquaculture applications, concentration is often reported in milligrams of gaseous oxygen per litre of water (mg / L) (Bittig et al. 2018), which is equivalent to parts per million (ppm).\nPercent saturation describes how “full” of oxygen the water is (Equation 1). The maximum amount of oxygen that can be dissolved depends on the water temperature, salinity, and barometric pressure (Figure 1, Figure 2). For units of concentration, this theoretical maximum is called the DO solubility, and is typically calculated from equations based on those developed by Benson and Krause (1980, 1984) and Garcia and Gordon (1992)1 Cloern (1999). The percent saturation is the measured DO concentration as fraction of the DO solubility (Equation 1).\n\\[\nDO_{\\%saturation} = 100 * DO_{concentration}/DO_{solubility}\n\\tag{1}\\]\nPercent saturation can also be calculated from partial pressures, although DO is not typically expressed in pressure units (Equation 2). In this case, the maximum amount of DO that can be dissolved is the partial pressure of oxygen in the air (\\(pO_{2, air}\\)), as calculated in SCOR WG 142 (Bittig 2018). The actual amount of DO in the water is measured by the sensor in corresponding pressure units (\\(pO_2\\)).\n\\[\nDO_{\\%saturation} = 100 * pO_2 / pO_{2, air}\n\\tag{2}\\]\n\n\n\n\n\nFigure 1: Relationship between DO solubility and temperature at five values of salinity (pressure = 1 atm). Note that DO solubility decreases with increasing temperature and salinity.\n\n\n\n\n\n\n\n\n\nFigure 2: Relationship between DO solubility and temperature at three values of barometric pressure (in freshwater). Note that DO solubility increases with increasing pressure.\n\n\n\n\nConversion between DO units can be non-trivial (Bittig 2018). DO solubility is used to convert between units of concentration and percent saturation, which means estimates of temperature, salinity, and pressure are required. For the most accurate conversion, each DO observation should have a corresponding observation of these water properties. However, this is not always feasible, particularly for long deployments. Single value estimates can be used to convert data from the whole deployment (Onset 2014)."
  },
  {
    "objectID": "pages/do_overview.html#supersaturation",
    "href": "pages/do_overview.html#supersaturation",
    "title": "Overview",
    "section": "Supersaturation",
    "text": "Supersaturation\nWhen the measured DO concentration is greater than the DO solubility (\\(DO_{\\%saturation} > 100 \\%\\)), the water is considered “supersaturated”. Supersaturated water is typically caused by photosynthetic organisms, which are a large source of pure oxygen to the water column (YSI 2019; Craig and Hayward 1987) 2. Additionally, a rapid increase in temperature can decrease the DO solubility without altering the measured DO concentration, resulting in saturation values greater than 100 %3. The excess DO will eventually diffuse into the atmosphere, but this process is not instantaneous."
  },
  {
    "objectID": "pages/do_overview.html#biofouling",
    "href": "pages/do_overview.html#biofouling",
    "title": "Overview",
    "section": "Biofouling",
    "text": "Biofouling\nA major challenge of accurately measuring DO near the surface4 is the growth of aquatic organisms on and around the sensor, called biofouling (OOI 2022). Biofouling can range from a small film of algae, to large colonies/growths of seaweed, to colonies of mollusks (Figure 3). The daily cycle of photosynthesis (oxygen production during the day) and respiration (oxygen consumption, relatively higher at night) of these organisms can cause extreme variability in the DO measurements. The recorded DO therefore reflects the microcosm growing on the sensor, but not the ambient environment. This signal can occur within a month or two of deployment (OOI 2022), depending on time time of year and location. Other impacts of biofouling can include signal attenuation, sensor drift, and decreased mooring depth from the additional weight (IOOS 2018).\n\n\n\nFigure 3: Examples of biofouling on CMAR sensor strings.\n\n\n\n\n\nAnti-fouling strategies can be employed to reduce fouling and improve data quality, although these present other challenges. Sensors can be cleaned regularly to remove growth (e.g., every 2 - 4 weeks), although this may not be feasible for many reasons (e.g., cost and time constraints). Some sensors have built-in anti-fouling mechanisms such as wipers (PME 2023) or UV light (Mariscope 2020) to reduce growth, but these sensors tend to be expensive and have reduced battery life. Copper (a natural biocide) tape, wire, or screens around the sensor can reduce fouling for several weeks, although this can also become expensive and may not be sufficient for longer deployments (YSI 2023)."
  },
  {
    "objectID": "pages/do_overview.html#other-challenges",
    "href": "pages/do_overview.html#other-challenges",
    "title": "Overview",
    "section": "Other Challenges",
    "text": "Other Challenges\nQuality control of DO data can also be challenging and time consuming. There is a broad range of “reasonable” DO values, depending on the location, depth, season, oceanographic conditions, etc. Rapid and extreme variability may reflect the natural processes that are of interest to the monitoring program, including phytoplankton blooms, storms, and upwelling. However, this signal could also be a result of biofouling. To identify the most likely driver, further investigation is required. For example, the timing of the variability with respect to sensor deployment, time of year, and co-located variables could provide insight into the reliability of the measurements. It is also difficult to detect and quantify sensor drift over time for a single deployment. Consecutive deployments at the same station can highlight drift (IOOS 2018)."
  },
  {
    "objectID": "pages/do_thresholds.html",
    "href": "pages/do_thresholds.html",
    "title": "Thresholds",
    "section": "",
    "text": "Most QC test thresholds for dissolved oxygen were based on historical Coastal Monitoring Program data. Preliminary quality control was applied to the data1. Different thresholds were calculated for data measured in percent saturation and concentration. The number of observations collected and number of counties and stations monitored for each set of units is presented in Table 1. Station locations are shown in Figure 1."
  },
  {
    "objectID": "pages/do_thresholds.html#gross-range-test",
    "href": "pages/do_thresholds.html#gross-range-test",
    "title": "Thresholds",
    "section": "Gross Range Test",
    "text": "Gross Range Test\n\nSensor Thresholds\nThe sensor thresholds were determined based on the associated manual (Table 2).\n\n\n\n\n\n\n\n\n\nUser Thresholds\n\nPercent Saturation\n\n\n\nA single set of user thresholds was determined for all counties because the DO records were relatively patchy in space and time (Figure 1; Figure 2), and because there was minimal variation between counties2 (Figure 3). For most counties, there were fewer “good” records in the spring and summer months, when conditions are conducive to biofouling3. Several counties have no data for at least one month (Figure 2), and 5 counties only have data from a single station (Figure 1). This analysis could be revisited and thresholds revised when there is more consistent data.\n\n\n\n\n\nFigure 2: The number of dissolved oxygen observations in each month for each county.\n\n\n\n\n\n\n\n\nFigure 3: The mean and standard deviation of dissolved oxygen (percent saturation) monthly data in each county.\n\n\n\nThree grouping methods were used to identify an appropriate mean and standard deviation on which to base the thresholds.\nMethod 1: the DO observations were grouped by calendar month to give equal weight to each month regardless of the number of observations (as described here for temperature). The average DO for each month was calculated, and the \\(avg_{DO_{Sat}}\\) and \\(stdev_{DO_{Sat}}\\) were calculated from these values.\nMethod 2: the DO observations were grouped by county to give equal weight to each county regardless of the number of observations. The average DO for each county was calculated, and the \\(avg_{DO_{Sat}}\\) and \\(stdev_{DO_{Sat}}\\) were calculated from these values.\nMethod 3: \\(avg_{DO_{Sat}}\\) and \\(stdev_{DO_{Sat}}\\) were calculated from all the observations, without any grouping.\nThe percent of observations flagged for each set of thresholds was used to inform the final choice of user thresholds.\n\n\n\n\n\n\n\n\n\n\n\nThe mean values for each grouping method were similar, but the standard deviations were not (Table 3). There was limited variability among the monthly means and the county means, resulting in relatively small standard deviations for Methods 1 and 2. This translates into very narrow user thresholds (Table 3; Figure 4).\nPreliminary quality control was applied to the data used in this analysis, and 12.03 % of these observations were still flagged using the Method 1 thresholds4. For some months, these thresholds flag values within within one standard deviation of the monthly mean. This would lead to many “false positive” flags, i.e., observations that are flagged as Suspect/Of Interest, despite being within a reasonable range.\nThe standard deviation for Method 35 is nearly twice as large as for the other Methods (Table 3). This leads to broader user thresholds and fewer false positive flags. Correspond to literature?. With these threshold values, 1.39 % of the observations were flagged.\nTo minimize the number of false positives, the thresholds based on Method 3 (all observations, ungrouped) will be used in QC analyses (Table 3).\n\n\n\n\n\nFigure 4: Distribution of dissolved oxygen observations (binwidth = 2 %; preliminary quality control applied). Observations flagged with the thresholds based on Method 1 are highlighted in light orange and dark orange. Observations flagged using the thresholds based on Method 3 are highlighted in dark orange.\n\n\n\n\nSubstantially more observations will be flagged from the raw data6. Some of these should be considered Of Interest (e.g., observations near the oxycline in Inverness, observations at depth in Guysborough), while others should be considered Suspect (e.g., biofouling). Human-in-the-loop review will be required to make these distinctions."
  },
  {
    "objectID": "pages/do_thresholds.html#climatology-test",
    "href": "pages/do_thresholds.html#climatology-test",
    "title": "Thresholds",
    "section": "Climatology Test",
    "text": "Climatology Test\nObservations from all counties were pooled to calculate a single set of climatology thresholds (Table 4). The \\(avg_{season}\\) was calculated as the average of all observations in a given month, and \\(stdev_{season}\\) was the associated standard deviation.\n\n\n\n\n\n\n\n\nFor some months the climatology threshold will trigger a Suspect/Of Interest flag before the Gross Range test (e.g., January - April); in other months, the Gross Range test will trigger the flag first (Figure 5).\n\n\n\n\n\nFigure 5: Monthly dissolved oxygen mean +/- 1 standard deviation. Solid orange lines indicate the climatology thresholds; dashed organge lines indicate the grossrange user thresholds."
  },
  {
    "objectID": "pages/do_thresholds.html#rate-of-change-test",
    "href": "pages/do_thresholds.html#rate-of-change-test",
    "title": "Thresholds",
    "section": "Rate of Change Test",
    "text": "Rate of Change Test\nObservations from all counties were pooled to calculate a single threshold for the rate of change test, \\(stdev_{max}\\).\nFirst, \\(stdev_{roll}\\) was calculated for each observation, and the distribution plotted (Figure 6).\n\nAB\n\n\n\n\n\n\n\nFigure 6: Distribution of the 24-hour rolling standard deviation of dissolved oxygen observations (binwidth = 0.25 %; preliminary quality control applied).\n\n\n\n\n\n\n\n\n\n\nFigure 7: Distribution of the 24-hour rolling standard deviation of dissolved oxygen observations (binwidth = 0.25 %; preliminary quality control applied). Filtered to show only standard deviations > 5.\n\n\n\n\n\n\n\nThe rolling standard deviation for most observations is less than 5. The higher standard deviations are possibly from biofouling signals that were not identified in the preliminary QAQC exercise, which was completed manually. This highlights one advantage of automated quality control, which can flag suspect observations that are more subtle than a human would detect.\nBased on Figure 6, a \\(stdev_{max}\\) of 4 or 5 would be reasonable.\nFigure 8 shows the percent of observations in each deployment that are flagged using a \\(stdev_{max}\\) threshold of of 4 or 5. As expect, the threshold of 4 flags more observations; the next step is to investigate whether the more strict threshold leads to more false positives. This step is in progress.\n\n\n\n\n\nFigure 8: Boxplots of the percent of observations flagged within each deployment, by county, using stdev_max = 4 and stdev_max = 5."
  },
  {
    "objectID": "pages/do_thresholds_roc.html",
    "href": "pages/do_thresholds_roc.html",
    "title": "Thresholds",
    "section": "",
    "text": "Observations from all counties were pooled to calculate a single threshold for the rate of change test, \\(stdev_{max}\\).\nFirst, \\(stdev_{roll}\\) was calculated for each observation, and the distribution plotted (?@fig-do-sd-roll-histogram).\n\nAB\n\n\n\n# \n# dat_roll <- dat_sat %>% \n#    select(\n#      county = COUNTY,\n#       deployment_range = DEPLOYMENT_PERIOD,\n#       waterbody = WATERBODY,\n#       station = STATION,\n#       sensor = SENSOR,\n#       timestamp_utc = TIMESTAMP,\n#       sensor_depth_at_low_tide_m = DEPTH,\n#       variable = VARIABLE,\n#       value = VALUE\n#     ) %>%\n#   separate(sensor, into = c(\"sensor_type\", \"sensor_serial_number\")) %>% \n#   mutate(variable = \"dissolved_oxygen_percent_saturation\") %>% \n#   ss_pivot_wider() %>%\n#   qc_test_rate_of_change(keep_sd_cols = TRUE) %>% \n#   na.omit()\n# \n# p <- plot_histogram(dat_roll, sd_roll, binwidth = 0.25) +\n#    scale_x_continuous(\"24-hour Standard Deviation (percent saturation)\") \n# \n# ggplotly(p)\n\n\n\n\n# p <- dat_roll %>% \n#   filter(sd_roll > 5) %>% \n#   plot_histogram(sd_roll, binwidth = 0.25) +\n#   scale_x_continuous(\n#     \"24-hour Standard Deviation (percent saturation)\",\n#     breaks = seq(5, 15, 1)\n#   ) \n#   \n# ggplotly(p)\n\n\n\n\n\nThe rolling standard deviation for most observations is less than 5. The higher standard deviations are possibly from biofouling signals that were not identified in the preliminary QAQC exercise, which was completed manually. This highlights one advantage of automated quality control, which can flag suspect observations that are more subtle than a human would detect.\nBased on ?@fig-do-sd-roll-histogram, a \\(stdev_{max}\\) of 4 or 5 would be reasonable.\n?@fig-do-sd-boxplot shows the percent of observations in each deployment that are flagged using a \\(stdev_{max}\\) threshold of of 4 or 5. As expect, the threshold of 4 flags more observations; the next step is to investigate whether the more strict threshold leads to more false positives. This step is in progress.\n\n#\n# roc_flags <- read_csv(\n#   here(\"pages/data/rate_of_change_flags.csv\"),\n#   show_col_types = FALSE\n# ) %>% \n#   mutate(diff = thresh_4 - thresh_5)\n\n# ggplot(roc_flags, aes(thresh_5, thresh_4, col = county)) +\n#   geom_point() +\n#   geom_abline(slope = 1, intercept = 0)\n# \n# ggplot(roc_flags, aes(county, diff)) +\n#   geom_boxplot()\n\n# \n# ggplot(roc_flags, aes(diff)) +\n#   geom_histogram()\n# \n# plot_histogram(roc_flags, diff, binwidth = 1)\n\n# \n# flags_long <- roc_flags %>% \n#   pivot_longer(\n#     cols = c(\"thresh_4\", \"thresh_5\"), \n#     names_to = \"stdev_max\", names_prefix = \"thresh_\", values_to = \"value\"\n#   )\n# \n# ggplot(flags_long, aes(county, value, fill = stdev_max)) +\n#   geom_boxplot(alpha = 0.5) +\n#   scale_fill_manual(values = c(\"#E78AC3\", \"#8DA0CB\") ) +\n#   scale_y_continuous(\"Percent of Observations Flagged as Suspect\") +\n#   scale_x_discrete(\"\") +\n#   theme(\n#     legend.position = c(0.8, 0.8),\n#     legend.box.background  = element_rect(colour = 1, linewidth = 1)\n#   )\n\n\n# ggplot(flags_long, aes(value, fill = stdev_max)) +\n#   geom_boxplot(alpha = 0.5, orientation = \"y\") +\n#   scale_fill_manual(values = c(\"#E78AC3\", \"#8DA0CB\") )"
  },
  {
    "objectID": "pages/qc_overview.html",
    "href": "pages/qc_overview.html",
    "title": "Overview",
    "section": "",
    "text": "CMAR applies automated and “human in the loop” quality control (QC) processes to the Coastal Monitoring Program data. It is beyond the scope of the Program to produce analysis-ready data products for all potential users, and some users may wish to apply additional QC.\n\n\nAn automated QC test is an algorithm that evaluates each data record and assigns a flag to the record indicating the test results. These flags are typically reviewed by human experts, which is referred to as “human in the loop” QC. End users can then filter the data set for records that meet their quality criteria (UNESCO 2013).\nNumerous QC flagging schemes and tests exist and have been applied to oceanographic data sets (e.g., Appendix A in UNESCO 2013). CMAR has adopted the well-known QARTOD flags and tests, which are applied by the U.S. Integrated Ocean Observing System (IOOS) and other ocean observing entities (see Table 1 of IOOS 2020a).\n\n\n\nQARTOD stands for the “Quality Assurance / Quality Control of Real-Time Oceanographic Data”. It is a project that grew from a 2003 grassroots effort to develop guidelines for high-quality oceanographic data. From 2012 to 2019, QARTOD developed 13 Quality Control manuals covering 14 ocean variables, plus additional supporting materials (IOOS 2020a). Each QC manual is subjected to three iterations of formal review by subject-matter experts, with the final round soliciting international reviews. Published manuals are updated as needed to reflect new technology, additional knowledge, or growth of the Project (IOOS 2020a).\n\n\nThe QARTOD flag scheme has 5 levels, as described in Table 1. More detail on how this flag scheme was developed is provided in UNESCO (2013).\n\n\nTable 1: QARTOD flag scheme (IOOS 2020b).\n\n\n\n\n\n\nFlag\nDescription\n\n\n\n\nPass - 1\nData have passed critical real-time quality control tests and are deemed adequate for use as preliminary data.\n\n\nNot evaluated - 2\nData have not been QC-tested, or the information on quality is not available.\n\n\nSuspect or Of High Interest- 3\nData are considered to be either suspect or of high interest to data providers and users. They are flagged suspect to draw further attention to them by operators.\n\n\nFail - 4\nData are considered to have failed one or more critical real-time QC checks. If they are disseminated at all, it should be readily apparent that they are not of acceptable quality.\n\n\nMissing data - 9\nData are missing; used as a placeholder.\n\n\n\n\n\n\n\nQARTOD manuals define QC tests for the 14 variables. For each variable, the tests are grouped into three categories: Required, Strongly Recommended, and Suggested. The Required tests provide the minimum level of QC, and should be easy to implement. However, it is recognized that there are circumstances where these tests are not applicable (IOOS 2020b). Codable instructions are included with the description of each test to facilitate implementation of automated data checking (IOOS 2020a).\n\n\nTable 2: QARTOD tests for temperature, salinity, and dissolved oxygen (IOOS 2020c, 2018).\n\n\nRequired\nTest 1\nGap Test\n\n\nRequired\nTest 2\nSyntax Test\n\n\nRequired\nTest 3\nLocation Test\n\n\nRequired\nTest 4\nGross Range Test\n\n\nRequired\nTest 5\nClimatological Test\n\n\nStrongly Recommended\nTest 6\nSpike Test\n\n\nStrongly Recommended\nTest 7\nRate of Change Test\n\n\nStrongly Recommended\nTest 8\nFlat Line Test\n\n\nSuggested\nTest 9\nMulti-Variate Test\n\n\nSuggested\nTest 10\nAttenuated Signal Test\n\n\nSuggested\nTest 11\nNeighbor Test\n\n\nSuggested1\nTest 12\nTS Curve/Space Test\n\n\nSuggested2\nTest 13\nDensity Inversion Test\n\n\n\n\nQARTOD manuals focus on QC of real-time data3, although the manuals acknowledge that other data types may also benefit from these flags and tests (IOOS 2020b). The CMAR Coastal Monitoring Program data is not processed in real-time. Instead, data is logged and offloaded for processing every 6 - 12 months. Some QARTOD tests were therefore not applicable to this data, and it was necessary and / or advantageous to modify some tests to reflect the nature of the data and processing.\nFor example, Test 1 and Test 2 are meant to identify gaps and syntax errors in real time so that the errors can be fixed and the record resume. They are therefore not applicable to the Coastal Monitoring Program data sets and were not implemented. Most of the remaining Required and Strongly Recommended tests were applied, but the Suggested tests are beyond the capacity of the current Data Governance team. These Suggested tests could be implemented in the future.\n\n\n\nQC tests require thresholds that determine the results of the test. Choosing appropriate thresholds for each test and variable is a key part of the QC effort. The operator (data provider) is responsible for selecting thresholds for QARTOD tests, and thresholds should be based on historical data when possible (IOOS 2020b; Taylor and Loescher 2013; OOI 2022)."
  },
  {
    "objectID": "pages/qc_tests.html",
    "href": "pages/qc_tests.html",
    "title": "QC Tests",
    "section": "",
    "text": "These tests will be applied to the December 2023 release of Water Quality Data\nThis page describes the QC tests applied to the Coastal Monitoring Program Water Quality data, and the general methods for selecting the most appropriate thresholds.\nWhere possible, the thresholds were determined from historical data, which provide a baseline of “normal” and “outlying” conditions. The historical data used here was the Coastal Monitoring Program Water Quality data sets submitted to the Nova Scotia Open Data Portal in December 2022. Preliminary quality control measures (e.g., obvious outliers and suspected biofouling removed) were applied to these datasets before submission. Additionally, freshwater and other outlier stations were excluded from the threshold analysis to provide a better representation of “normal” conditions.\nThese thresholds should be re-evaluated and re-calculated if necessary in several years, when more data is available."
  },
  {
    "objectID": "pages/qc_tests.html#gross-range-test",
    "href": "pages/qc_tests.html#gross-range-test",
    "title": "QC Tests",
    "section": "Gross Range Test",
    "text": "Gross Range Test\nThe Gross Range Test aims to flag observations that fall outside of the sensor measurement range (flagged Fail) and observations that are statistical outliers (flagged Suspect/Of Interest).\nThresholds for failed observations are named \\(sensor_{min}\\) and \\(sensor_{max}\\), and are determined by the sensor specifications. CMAR named assigned these thresholds for each variable and sensor based on information in the associated manuals.\nThresholds for suspect/of interest observations are named \\(user_{min}\\) and \\(user_{max}\\), and are defined by the data provider. CMAR assigned these thresholds based on historical Coastal Monitoring Program data.\nFollowing the OOI Biogeochemical Sensor Data: Best Practices & User Guide, these thresholds were calculated from historical data as the mean +/- three standard deviations (Equation 1, Equation 2):\n\\[\nuser_{min} = avg_{var} - 3 * stdev_{var}  \n\\tag{1}\\]\n\\[\nuser_{max} = avg_{var} + 3 * stdev_{var}\n\\tag{2}\\]\nwhere \\(avg_{var}\\) is average of the variable of interest, and \\(stdev_{var}\\) is the standard deviation of the variable of interest."
  },
  {
    "objectID": "pages/qc_tests.html#climatological-test",
    "href": "pages/qc_tests.html#climatological-test",
    "title": "QC Tests",
    "section": "Climatological Test",
    "text": "Climatological Test\nThe Climatological Test is a variation of the Gross Range Test that accounts for seasonal variability. There is no Fail flag associated with this test for temperature, salinity, or dissolved oxygen due to the dynamic nature of these variables (IOOS 2020, 2018). Observations that are seasonal outliers were assigned the flag Suspect/Of Interest.\nThese thresholds are named \\(season_{min}\\) and \\(season_{max}\\). The seasonal time period (e.g., monthly, seasonally, other) and associated thresholds are defined by the data provider. Following the OOI Biogeochemical Sensor Data: Best Practices & User Guide, seasons were defined based on the calendar month, and the thresholds were based on historical data. The monthly thresholds were defined similar to the Gross Range Test:\n\\[\nseason_{min} = avg_{season} - 3 * stdev_{season}  \n\\tag{3}\\]\n\\[\nseason_{max} = avg_{season} + 3 * stdev_{season}  \n\\tag{4}\\]\nThe \\(avg_{season}\\) was calculated as the average of all observations for a given month, and \\(stdev_{season}\\) was the associated standard deviation.\nNote that OOI used a more complex method (harmonic analysis, as described here) to estimate \\(avg_{season}\\) to account for spurious values. This was beyond the current scope of the CMAR Coastal Monitoring Program, but could be applied in future iterations of this threshold analysis."
  },
  {
    "objectID": "pages/qc_tests.html#spike-test",
    "href": "pages/qc_tests.html#spike-test",
    "title": "QC Tests",
    "section": "Spike Test",
    "text": "Spike Test\nText to come."
  },
  {
    "objectID": "pages/qc_tests.html#rate-of-change-test",
    "href": "pages/qc_tests.html#rate-of-change-test",
    "title": "QC Tests",
    "section": "Rate of Change Test",
    "text": "Rate of Change Test\nThe Rate of Change test applied to CMAR data was modified from the test described in the QARTOD documents (IOOS 2020, 2018). The QARTOD test compares the values of consecutive observations to a threshold defined by the operator. The threshold can be defined as a set value, or as \\(n_{dev}\\) (operator-specific) standard deviations, where the standard deviation is calculated over an operator-specified time frame. An observation is flagged as Suspect/Of Interest if:\n\\[ | T_n - T_{n-1} | > n_{dev} * stdev \\] where \\(T_n\\) is the current observation, and \\(T_{n-1}\\) is the previous observation.\nThis test is similar to the Spike Test, which compares the current observation the two adjacent observations.\nThe goal of the CMAR Rate of Change Test was to identify suspected biofouling in the dissolved oxygen data. This test was based on the rolling standard deviation, and should perhaps be renamed the Standard Deviation Test for clarity.\nThe test assumes that there is a 24-hour oxygen cycle, with net oxygen production during the day, and net oxygen consumption during the night. Biofouling is suspected when the amplitude of this cycle, as measured by the standard deviation, increases above a threshold (Figure 1).\n\n\n\n\n\nFigure 1: Simulated dissolved oxygen data. The green points (January 1 to January 15) represent no biofouling; the orange points represent biofouling. The mean value and amplitudes are based on data observed from the Coastal Monitoring Program.\n\n\n\nThe standard deviation, \\(stdev_{roll}\\), was calculated from a 24-hour centered rolling window of observations, i.e., \\(T_{n-m/2}\\), … \\(T_{n-1}\\), \\(T_{n}\\), \\(T_{n+1}\\), … \\(T_{n+m/2}\\).1 The number of observations in each window depends on the sample interval, which is typically 10 minutes.\nAlthough this test was designed to identify suspected biofouling, it was also applied to the other Water Quality variables as a general Rate of Change test. In particular, it is expected to flag rapid changes in temperature due to fall storms.\nFollowing the QARTOD Rate of Change Test, this test does not flag any observations as fail because of the high natural variable in dissolved oxygen, temperature, and salinity. Observations with a standard deviation that exceeds the threshold are flagged as Suspect/Of Interest. Observations at the beginning and end of the deployment for which the rolling standard deviation cannot be calculated (i.e, observations less than 12 hours from the start or end of deployment) are flagged Not evaluated."
  },
  {
    "objectID": "pages/qc_tests.html#flat-line-test",
    "href": "pages/qc_tests.html#flat-line-test",
    "title": "QC Tests",
    "section": "Flat Line Test",
    "text": "Flat Line Test\nText to come."
  },
  {
    "objectID": "pages/temp_measurements.html",
    "href": "pages/temp_measurements.html",
    "title": "CMAR Measurements",
    "section": "",
    "text": "CMAR has collected temperature data from 126 stations in 15 counties (Figure 1).\n\n\n\n\n\nFigure 1: Approximate location of stations with temperature data.\n\n\n\n\n\n\nA large proportion of these records are from Guysborough County (37.24 %), while a small proportion are from Cape Breton (0.11 %) and Colchester (0.6 %) Counties (Figure 2).\n\n\n\n\n\nFigure 2: The number of temperature observations in each county."
  },
  {
    "objectID": "pages/temp_measurements.html#depth",
    "href": "pages/temp_measurements.html#depth",
    "title": "CMAR Measurements",
    "section": "Depth",
    "text": "Depth\nThe placement of the temperature sensors depends on the depth of the water at each station. Typically, sensors are fastened 2, 5, 10, 15, and 20 m below the surface at low tide. At deeper stations, additional sensors are added every 5 - 10 m. At shallow stations, sensors maybe be attached nearer to the surface (Figure 3).\nSensors may be placed at other depths for technical reasons or specific research projects (e.g., investigation of the oxycline in Whycocomagh Basin).\n\n\n\n\n\nFigure 3: Number of temperature observations at each depth (rounded to nearest whole number). Note that only depths with measurements are shown."
  },
  {
    "objectID": "pages/temp_measurements.html#sensors",
    "href": "pages/temp_measurements.html#sensors",
    "title": "CMAR Measurements",
    "section": "Sensors",
    "text": "Sensors\nCMAR uses several types of sensors to measure temperature (Table 1):\n\n\n\n\n\n\n\n\nSome of these sensors also measure other variables including dissolved oxygen, salinity, depth, and acoustic detections.\nThe Vemco VR2AR is typically the deepest sensor, anchored about 0.5 m above the sea floor. It has an acoustic release that is triggered to retrieve the sensor string. Deployments without VR2AR sensors are usually accessible from the surface, but some are retrieved by dragging or divers."
  },
  {
    "objectID": "pages/temp_overview.html",
    "href": "pages/temp_overview.html",
    "title": "Overview",
    "section": "",
    "text": "Text to come.\n\n\n\n\n\nImage by Jose R. Cabello from Pixabay"
  },
  {
    "objectID": "pages/temp_thresholds.html",
    "href": "pages/temp_thresholds.html",
    "title": "Thresholds",
    "section": "",
    "text": "Most QC test thresholds for temperature were based on historical Coastal Monitoring Program data. Preliminary quality control was applied to this data1, leaving observations from 122 stations in 15 counties (Figure 1)."
  },
  {
    "objectID": "pages/temp_thresholds.html#gross-range-test",
    "href": "pages/temp_thresholds.html#gross-range-test",
    "title": "Thresholds",
    "section": "Gross Range Test",
    "text": "Gross Range Test\n\nSensor Thresholds\nThe sensor thresholds were determined based on the associated manual (Table 1).\n\n\n\n\n\n\n\n\n\nUser Thresholds\nUser thresholds were calculated separately for each county due to expected and observed spatial differences in temperature (Figure 2).\n\n\n\n\n\nFigure 2: The mean and standard deviation of temperature in each county.\n\n\n\nMost counties have substantially different number of observations for different seasons (Figure 5). Because temperature has a clear seasonal cycle (Figure 4), this can weight the average towards months with more observations. To give each month equal weight, the user thresholds were based on the monthly climatology2.\nAll temperature observations were first grouped by calendar month, and the average temperature for each month was calculated. The \\(avg_{Temp}\\) was the average of these monthly averages, and \\(stdev_{Temp}\\) was the standard deviation of the monthly averages (Equation 1, Equation 2). With this approach, the mean for a given month will be weighted towards years with more observations than other years for that month. This is not expected to have a substantial influence on the calculated thresholds, but future iterations of this exercise could further standardize the data to account for this.\n\\[\navg_{temp} = sum(avg_{Jan} + avg_{Feb} + ... avg_{Dec}) / 12\n\\tag{1}\\]\n\\[\nstdev_{temp} = sd(avg_{Jan}, avg_{Feb}, ... avg_{Dec})\n\\tag{2}\\]\nCounty statistics and user thresholds are presented in Table 2.\n\n\n\n\n\n\n\n\n\nThe quality3 of these user thresholds may vary by county, depending on the number and distribution (in space and time) of observations. For example, there are relatively few observations for some counties compared to others (Figure 3). Cape Breton has the fewest observations at ~30,000 over 3 years and two stations, while Guysborough has the most at nearly 1 million over 7 years and 35 stations (Figure 1). Colchester and Queens counties only have one station each (Figure 1). For consistency, the counties with fewer observations were not pooled with counties with more observations. The user thresholds should be re-evaluated when more observations are collected.\n\n\n\n\n\nFigure 3: The number of dissolved oxygen observations in each county. (Note these numbers are not the same as on the CMAR measurements page because some outlier values and stations were omitted from the thresholds analysis.)\n\n\n\nCalculating thresholds at the county scale provides relatively coarse threshold values. Ideally, these would be resolved by depth and a smaller spatial scale (e.g., waterbody or station). However, calculating thresholds for each county and depth provides its own challenges. The data become very patchy when grouped by county, depth, and month (e.g., 169 month-county-depth combinations with 0 observations). Additionally, the same depth can represent a different part of the water column for different stations. At the Barren Island station in Guysborough county, the 15 m sensor is near the bottom. In contrast, 15 m is in the top 20 % of the water column at Tickle Island, also in Guysborough county. Finally, aggregating thresholds by county and depth would result in 141 user-defined temperature thresholds, which is more than the 2.5 person Data Governance team can reasonably manage.\nBecause depth was not accounted for, it is expected that observations from very shallow sensors (e.g., <= 2 m) will be assigned the Suspect/Of Interest flag, despite appearing reasonable in the context of the deployment. In this case, the flag should be interpreted as “Of Interest”, for highlighting a relatively warm observation.\nThe \\(user_{min}\\) threshold is typically << 0 degrees Celsius (Table 2), and is therefore not expected to flag any observations. For most counties (all except Annapolis, Queens, Shelburne, and Digby), the \\(user_{min}\\) is less than the \\(sensor_{min}\\) for the aquameasure and vr2ar sensors. In this case, any observations less than the \\(sensor_{min}\\) would fail the Gross Range Test (i.e., the \\(user_{min}\\) would be ignored). It may be useful for other users to apply their own \\(user_{min}\\) threshold to highlight cold observations that are Suspect/Of Interest. For example, those interested in salmonid aquaculture may wish to flag observations at or near the superchill threshold (-0.7 degree C)."
  },
  {
    "objectID": "pages/temp_thresholds.html#climatological-test",
    "href": "pages/temp_thresholds.html#climatological-test",
    "title": "Thresholds",
    "section": "Climatological Test",
    "text": "Climatological Test\nThe season thresholds were calculated separately for each county due to expected and observed spatial differences in temperature (Figure 4).\n\nAB\n\n\n\n\n\n\n\nFigure 4: The monthly mean and standard deviation of temperature in each county.\n\n\n\n\n\n\n\n\n\nThe monthly mean and standard deviation of temperature in each county.\n\n\n\n\n\nSeasonal statistics and thresholds are presented in Table 3. The quality4 of these thresholds may vary by county and month, depending on the number and distribution (in space and time) of observations. There were no county-month groups with zero observations, although some groups had relatively few observations for many or all months (Figure 5; e.g., Cape Breton).\n\n\n\n\n\n\n\n\n\n\nSpatial & Temporal Resolution\nThe length of the time series varied among counties, from approximately 1 year (e.g., Cape Breton, Colchester) to over 7 years (select stations in Guysborough, Richmond, and Yarmouth counties; Figure 6). This means that the monthly average for some county-month groups is based on only 1 year of data.\nThe number of stations varied by county, from a single station each in Colchester and Queens counties, to 34 stations in Guysborough (Figure 1).\nThe seasonal thresholds for counties with limited years, number of observations, and/or number of stations are likely not representative of inter-annual or spatial variability in the county; however, they are adequate for this quality control exercise. These thresholds are representative of normal conditions of the observed deployments, and so outlying values will be flagged. If new deployments are added to these counties, it is recommended that the seasonal thresholds be re-evaluated and re-calculated if necessary. It is strongly recommended that all seasonal thresholds be re-calculated after several more years of data have been collected through the Coastal Monitoring Program.\n\nFigure 5Figure 6\n\n\n\n\n\n\n\nFigure 5: The number of temperature observations in each month for each county.\n\n\n\n\n\n\n\n\n\n\nFigure 6: The number of years of temperature data for each month and county.\n\n\n\n\n\n\n\n\nDepth & Stratification\nLike the user thresholds, the seasonal thresholds were not resolved by depth. This means that there was high standard deviation for counties with seasonal stratification, particularly in the summer months when stratification is typically the strongest (Figure 4; Table 3).\nThe standard deviations for July through October in Inverness are the four highest overall standard deviations (Table 3). This high variability was driven by three deployments in Whycocomagh Basin in the Bras D’Or Lakes:\n\nDeep Basin (May to September 2018)\n0814x East (September to December 2020)\n0814x West (September to December 2020)\n\nThese stations had sensors deployed above and below the thermocline. Temperatures below the thermocline, near the bottom, were typically very cold (about zero degrees for the whole Deep Basin deployment). Temperatures above the thermocline, closer to the surface, were typically much warmer, up to 25 °C in the summer (link to Inverness report here). The high standard deviation results in a very wide range of temperature values that would be flagged Pass: e.g., from -11.14 °C to 36.578 °C in August.\nThe temperatures below the thermocline could be considered anomalous and removed prior to calculating the thresholds. These temperatures would then be flagged Of Interest. However, the temperatures were included in the current thresholds analysis for several reasons:\n\nTogether, these three deployments represent 68 % of the Inverness temperature observations (those from below the thermocline represents 30 % of the county observations).\nConsistency with the threshold calculation for other counties (e.g., depth was not accounted for).\nIt is not suspicious for these temperatures to be so cold in this region of Whycocomagh Basin.\n\nThere are stations withe notable depth stratification in other counties, including Guysborough, Halifax, and Lunenburg. This is reflected in the relatively high standard deviation in the summer months for these counties (Figure 4; Table 3). Future iterations of this threshold analysis could consider resolving the seasonal thresholds by depth; however it is beyond the scope of the current exercise.\n\n\nUser vs. Seasonal Thresholds"
  }
]